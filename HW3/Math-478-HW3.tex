\documentclass[10pt]{report}

\usepackage{subcaption} % for subfigures
\usepackage{amsthm} % for QED
%\usepackage{algpseudocode} % for pseudo-code
\usepackage{mathtools} % for delimiter

\usepackage{listings} % for code
\lstset
{
	language=Matlab,
	frame=single,
	basicstyle=\footnotesize,
	captionpos=b,
	numbers=left,
	stepnumber=1,
	showstringspaces=false,
	tabsize=4,
	breaklines=true,
	breakatwhitespace=false,
}

\usepackage{siunitx} % for scientific notation
% for `e' in scientific notation
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

\usepackage{float} % for figure [H]
\usepackage{booktabs} % for tabular
\usepackage{caption} % for \caption*
\usepackage[export]{adjustbox} % for valign=t
\usepackage{array} % for column type m
\usepackage{verbatim}
\usepackage{graphicx}
\graphicspath{ {imgs/} }
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}

%%%%%% Pagination
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}

%Title page
\newcommand{\hwTitle}{Homework \#3}
\newcommand{\hwCourse}{Numerical Differential Equations/Computational Mathematics II}
\newcommand{\hmwkClassInstructor}{Professor Shuwang Li}

\title{
	\vspace{2in}
	\textmd{\textbf{\hwCourse\\\hwTitle}}\\
	\vspace{0.3in}\large{\textit{\hmwkClassInstructor}}
	\vspace{3in}
}

%\title{Homework 1}
\author{\textbf{Zhihao Ai}}
\date{}

%Header setting. 
\pagestyle{fancy}
\fancyhead[L]{Zhihao Ai}
\fancyhead[C]{Math 478}
\fancyhead[R]{Homework 3}
%%%%%%

%Global setting.
%\everymath{\displaystyle}
\setlength\parindent{0pt}

%Custom general commands.
\newcommand{\ds}{\displaystyle}
\newcommand{\ts}{\textstyle}
\newcommand{\f}[1] {f\left(#1\right)}
\newcommand{\eva}[2] {\left. #1 \right|_{#2}}
\newcommand{\dintt}[4] {\int_{#1}^{#2} #3 d#4}

\newcolumntype{N}{ >$ c <$}
\newcolumntype{M}[1]{>{\centering\arraybackslash $}m{#1}<{$}}

\newcommand{\abs}[1] {\left| #1 \right|}

\DeclarePairedDelimiter\autoparen{(}{)}
\newcommand{\pa}[1]{\autoparen*{#1}}
\DeclarePairedDelimiter\autodvert{\Vert}{\Vert}
\newcommand{\norm}[1]{\autodvert*{#1}}

\begin{document}

\maketitle

\section*{Question 1}
% Fundamental concepts on solving nonlinear equation(s).
\begin{enumerate}
	\item 
	(Section 3.2 Problem 5) What is the purpose of the following iteration formula?
	\[
	x_{n+1} = 2x_n - x_n^2 y
	\]
	Identify it as the Newton iteration for a certain function.
	
	According to Newton iteration formula,
	\begin{align*}
		x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} = 2x_n - x_n^2 y &= x_n - (x_n^2 y - x_n)\\
		\to \frac{y_n}{y'_n} &= x_n^2y_n - x_n
	\end{align*}
	So, the formula is to find the root of $y(x)$ where $y/y' = x^2y - x$ by Newton's method.
	
	\item 
	(Section 3.2 Problem 13) If Newton's method is used with $f(x)=x^2-1$ and $x_0 = 10^{10}$, how many steps are required to obtain the root with accuracy $10^{-8}$? (Solve analytically, not experimentally.)
	
	By Newton iteration formula,
	\[
	x_{n+1} 
	= x_n - \frac{f(x_n)}{f'(x_n)} 
	= x_n - \frac{x_n^2 - 1}{2x_n} 
	= \frac{1}{2}\left(x_n + \frac{1}{x_n}\right)
	\]
	
	Since $x_0 > 1$ and the closest root to $x_0$ is $x = 1$, $x_{n} > 1$ on $(1, x_0]$. Therefore, $\frac{1}{x_n} < 1$ and $x_{n+1} < \frac{1}{2}(x_n + 1)$.
	Given $x_1 < \frac{1}{2} (x_0-1) + 1$, assume $x_n < \frac{1}{2^n}(x_0 - 1) + 1$. For $x_{n+1}$,
	\[
	x_{n+1} 
	< \frac{1}{2}(x_n + 1) 
	< \frac{1}{2}\left(\frac{1}{2^n}(x_0 - 1) + 1 + 1\right) 
	= \frac{1}{2^{n+1}}(x_0 - 1) + 1
	\]
	Therefore $ x_n < \frac{1}{2^n}(x_0 - 1) + 1$ holds by induction. Since $f(x)$ is convex ($f''(x) = 2 > 0$) and monotonically increasing on $(0,\infty)$, $x_n$ converges to the closest root $x=1$. To obtain accuracy within  $10^{-8}$, we need
	\[
	\frac{1}{2^n}(x_0 - 1) + 1 \le 1 + 10^{-8}
	\]
	The smallest $n\in\mathbb{N}$ that satisfies the inequality is $n=60$. Therefore, analytically, 60 steps are required.
	
	\item 
	(Section 3.2 Problem 22) Starting with $(0,0,1)$, carry out an iteration of Newton's method for nonlinear systems on
	\[
	\left\{
	\begin{aligned}
	xy - z^2 &= 1\\
	xyz- x^2 + y^2 &= 2\\
	e^x - e^y + z &= 3
	\end{aligned}
	\right.
	\]
	Explain the results.
	
	Define
	\[
	F(X) 
	= \begin{pmatrix}
	f_1(x_1, x_2, x_3)\\
	f_2(x_1, x_2, x_3)\\
	f_3(x_1, x_2, x_3)
	\end{pmatrix}
	= \begin{pmatrix}
		x_1 x_2 - x_3^2 - 1\\
		x_1 x_2 x_3 - x_1^2 + x_2^2 - 2\\
		e^{x_1} - e^{x_2} + x_3 - 3
	\end{pmatrix}
	\]
	Take the partial derivatives,
	\[
	F'(X)
	= \begin{pmatrix}
	x_2 & x_1 & -2x_3\\
	x_2 x_3 - 2x_1 & x_1x_3 + 2x_2 & x_1 x_2\\
	e^{x_1} & -e^{x_2} & 1
	\end{pmatrix}
	\]
	For the first iteration,
	\[
	F'(X^{(0)})
	= \begin{pmatrix}
	0 & 0 & -2\\
	0 & 0 & 0\\
	1 & -1 & 1
	\end{pmatrix}
	\]
	Since the Jacobian matrix is singular for $X^{(0)}$, $F'(X^{(0)}) H^{(0)} = -F(X^{(0)})$ cannot be solved. Without $H^{(0)}$, the iteration cannot proceed.
	
	\item 
	Use Newton's method to estimate the value of $7^{1/5}$ up to 8-digit accuracy.
	
	Let $x = 7^{1/5}$ be the root of $f(x) = x^5 - 7$. By Newton iteration formula
	\[
	x_{n+1} 
	= x_n - \frac{f(x_n)}{f'(x_n)}
	= \frac{4}{5}x_n + \frac{7}{5x_n^4}
	\]
	Since $7^{1/5} \approx 1.47577316$, consider $x_0 = 1.5$ and we have
	\[
	\begin{tabular}{*{2}{N}} 
		\toprule
		i & x_i \\ \midrule
		0 & 1.50000000\\
		1 & 1.47654321\\
		2 & 1.47577396\\
		3 & 1.47577316\\
		\bottomrule
	\end{tabular}
	\]
	
\end{enumerate}

\section*{Question 2}
% Fundamental concepts on IVP.
Given IVP: $\frac{dy}{dt} = t^2(y+2), y(0)=1$ with domain $D=\{(t,y) | 0\le t\le 2, -\infty<y<\infty\}$
\begin{enumerate}
	\item
	Show that $f(t,y) = t^2(y+2)$ satisfies a Lipschitz condition and find a Lipschitz constant.
	
	$\forall (t, y_1), (t, y_2) \in D$, 
	\[
	\abs{f(t,y_1) - f(t,y_2)}
	= \abs{t^2(y_1 + 2) - t^2(y_2 + 2)}
	= t^2\abs{y_1 - y_2}
	\]
	Since $t\in [0,2]$, $t^2\le 4$ and we have
	\[
	\abs{f(t,y_1) - f(t,y_2)}
	\le 4\abs{y_1 - y_2}
	\]
	Therefore, $f(t,y)$ satisfies a Lipschitz condition and a Lipschitz constant is 4.
	
	\item
	$y(t)=3e^{t^3/3} - 2$ solves the IVP problem. Show that the IVP is well-posed by considering that $f(t,y)$ is perturbed by adding a term $\delta(t) = 2\epsilon t^2$ and the initial condition is perturbed by adding a term $\epsilon_0$ with $\abs{\epsilon_0}\le\epsilon$; i.e. show that $\abs{y_{perturb}(t) - y(t)}\le K\epsilon$ where $K$ is a constant.
	
	Denote the perturbed $y(t)$ as $z(t)$,
	\[
	\left\{
	\begin{aligned}
	\frac{dz}{dt} &= t^2(z+2) + 2\epsilon t^2 = t^2(z+2+2\epsilon)\\
	z(0) &= 1 + \epsilon_0
	\end{aligned}
	\right.
	\]
	Solving the first equation we get $z(t) = C e^{t^3/3} - 2 -2\epsilon$. Plug it in the second equation, we obtain $C = 3+\epsilon_0+2\epsilon$ and $z(t) = (3+\epsilon_0+2\epsilon)e^{t^3/3} - 2 -2\epsilon$. Therefore,
	\begin{align*}
	\abs{z(t) - y(t)} &= \abs{(3+\epsilon_0+2\epsilon)e^{t^3/3} - 2 -2\epsilon - 3e^{t^3/3} + 2}\\
	&= \abs{(\epsilon_0+2\epsilon)e^{t^3/3} - 2\epsilon}\\
	&\le \abs{3\epsilon e^{t^3/3} - 2\epsilon} \tag{$\abs{\epsilon_0}\le\epsilon$}\\
	&= \abs{3e^{t^3/3}-2} \epsilon \tag{$\epsilon > \abs{\epsilon_0} \ge 0$}\\
	&\le (3e^{8/3} - 2) \epsilon \tag{$t\in [0,2]$}
	\end{align*}
	Since $3e^{8/3} - 2$ is constant, the IVP is well-posed.
	
\end{enumerate}

\section*{Question 3}
% Fundamental concepts on Euler's method and Taylor series method.
\begin{enumerate}
	\item 
	(Problem 1.1) Apply the method of proof of Theorems 1.1 and 1.2 to prove the convergence of the implicit midpoint rule (1.12) and of the theta method (1.13).
	
	Proof of the convergence of the implicit midpoint rule:\\
	Denote the error as $\eta$ and substitute the exact solution,
	\[
	y(t_{n+1}) = y(t_n) + hf\pa{t_n+\frac{1}{2}h, \frac{1}{2}(y(t_n) + y(t_{n+1}))} + \eta	\tag{1}
	\]
	Modify $f(\cdot)$ and we have
	\[
	hf\pa{t_n+\frac{1}{2}h, \frac{1}{2}(y(t_n) + y(t_{n+1}))}
	= hf\pa{t_n+\frac{1}{2}h, y\pa{t_n+\frac{1}{2}h}} + \eta_1	\tag{2}
	\]
	By Lipschitz condition,
	\begin{align*}
	\abs{\eta_1} 
	&= h\norm{f\pa{t_n+\frac{1}{2}h, \frac{1}{2}(y(t_n) + y(t_{n+1}))} - f\pa{t_n+\frac{1}{2}h, y\pa{t_n+\frac{1}{2}h}}}\\
	&\le \frac{1}{2}h\lambda \norm{y(t_n) + y(t_{n+1}) - 2y\pa{t_n+\frac{1}{2}h}}
	\end{align*}
	Apply Taylor expansion,
	\[
	\abs{\eta_1} 
	\le \frac{1}{2}h\lambda \norm{y(t_n) + y(t_n) + hy'(t_n) - 2\pa{y(t_n) + \frac{1}{2}hy'(t_n)} + O(h^2)}
	= O(h^3)	\tag{3}
	\]
	Plug (2), (3) in (1),
	\begin{align*}
	\abs{\eta}
	&= y(t_{n+1}) - y(t_n) - hf\pa{t_n+\frac{1}{2}h, y\pa{t_n+\frac{1}{2}h}} + O(h^3)\\
	&= \pa{y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n)} - y(t_n) - h\pa{y'(t_n) + \frac{1}{2}hy''(t_n) + O(h^2)} + O(h^3) = O(h^3)
	\end{align*}
	Subtract the trapezoidal rule from the exact solution,
	\begin{align*}
		e_{n+1} &= y(t_{n+1}) - y_{n+1}\\
		&= y(t_n) + hf\pa{t_n + \frac{1}{2}h, \frac{1}{2}(y(t_n) + y(t_{n+1}))} + O(h^3) - \pa{y_n + hf\pa{t_n + \frac{1}{2}h, \frac{1}{2}(y_n + y_{n+1})}}\\
		&= e_n + h\pa{f\pa{t_n + \frac{1}{2}h, \frac{1}{2}(y(t_n) + y(t_{n+1}))} - f\pa{t_n + \frac{1}{2}h, \frac{1}{2}(y_n + y_{n+1})}} + O(h^3) 
	\end{align*}
	For analytic $f$ we can bound the $O(h^3)$ term by $ch^3$ for some $c > 0$. It follows from the Lipschitz condition and the triangle inequality that
	\begin{align*}
	\norm{e_{n+1}} &\le \norm{e_n} + h\norm{f\pa{t_n + \frac{1}{2}h, \frac{1}{2}(y(t_n) + y(t_{n+1}))} - f\pa{t_n + \frac{1}{2}h, \frac{1}{2}(y_n + y_{n+1})}} + ch^3\\
	&\le \norm{e_n} + \frac{1}{2}h\lambda \norm{e_n + e_{n+1}} + ch^3\\
	&\le \norm{e_n} + \frac{1}{2}h\lambda \norm{e_n} + \frac{1}{2}h\lambda\norm{e_{n+1}} + ch^3\\
	&\le \frac{1+h\lambda/2}{1-h\lambda/2} \norm{e_n} + \frac{1}{1-h\lambda/2}ch^3
	\end{align*}
	Assume that
	\[
	\norm{e_n} \le \frac{c}{\lambda} \pa{\pa{\frac{1+h\lambda/2}{1-h\lambda/2}}^n - 1}h^2
	\]
	By induction, $e_0 = 0 \le 0$ so it is true for $n=0$. For $n+1$,
	\[
	\norm{e_{n+1}} \le \pa{\frac{1+h\lambda/2}{1-h\lambda/2}} \frac{c}{\lambda} \pa{\pa{\frac{1+h\lambda/2}{1-h\lambda/2}}^n - 1}h^2 + \frac{1}{1-h\lambda/2}ch^3 = \frac{c}{\lambda} \pa{\pa{\frac{1+h\lambda/2}{1-h\lambda/2}}^{n+1} - 1}h^2
	\]
	So the assumption is true. By a Taylor expansion of the exponential function,
	\[
	\frac{1+h\lambda/2}{1-h\lambda/2} 
	= 1+\frac{h\lambda}{1-h\lambda/2}
	\le \sum_{l=0}^{\infty} \frac{1}{l!} \pa{\frac{h\lambda}{1-h\lambda/2}}^l
	= \exp\pa{\frac{h\lambda}{1-h\lambda/2}}
	\]
	$\norm{e_n}$ can be expressed as 
	\[
	\norm{e_n} 
	\le \frac{ch^2}{\lambda} \pa{\frac{1+h\lambda/2}{1-h\lambda/2}}^n
	\le \frac{ch^2}{\lambda} \exp\pa{\frac{nh\lambda}{1-h\lambda/2}}
	\le \frac{ch^2}{\lambda} \exp\pa{\frac{t^*\lambda}{1-h\lambda/2}}
	\]
	Therefore,
	\[
	\lim\limits_{\substack{h\to 0\\0\le nh\le t^*}} \norm{e_n} = 0
	\]
	Hence the implicit midpoint rule converges.
	
	Proof of the convergence of the theta method:
	\[
	y_{n+1} = y_n + h(\theta f(t_n,y_n) + (1-\theta)f(t_{n+1}, y_{n+1})) \tag{1}
	\]
	According to Iserles, 
	\begin{multline}
	y(t_{n+1}) = y(t_n) + h(\theta f(t_n,y(t_n)) + (1-\theta)f(t_{n+1}, y(t_{n+1})) +\\
	\pa{\theta - \frac{1}{2}}h^2y''(t_n) + \pa{\frac{1}{2}\theta - \frac{1}{3}}h^3y'''(t_n) + O(h^4) \tag{2}
	\end{multline}
	If $\theta = \frac{1}{2}$, the method reduces to trapezoidal rule, which converges; so consider only $\theta\ne\frac{1}{2}$ . Subtract (2) from (1), 
	\begin{align*}
		e_{n+1}
		&= y_{n+1} - y(t_{n+1})\\
		&= y_n + h(\theta f(t_n,y_n) + (1-\theta)f(t_{n+1}, y_{n+1})) - y(t_n) - h(\theta f(t_n,y(t_n)) - (1-\theta)f(t_{n+1}, y(t_{n+1}))\\
		&\phantom{=\ } - \pa{\theta - \frac{1}{2}}h^2y''(t_n) - \pa{\frac{1}{2}\theta - \frac{1}{3}}h^3y'''(t_n) + O(h^4)\\
		&= e_n + h\theta \pa{f(t_n,y_n) - f(t_n,y(t_n))} + h(1-\theta)\pa{f(t_{n+1},y_{n+1}) - f(t_{n+1},y(t_{n+1}))}\\
		&\phantom{=\ } - \pa{\theta - \frac{1}{2}}h^2y''(t_n) + O(h^3)
	\end{align*}
	For analytic $f$ we can bound the $O(h^2)$ term by $ch^2$ for some $c > 0$. It follows from the Lipschitz condition and the triangle inequality that
	\begin{align*}
	\norm{e_{n+1}} 
	&\le \norm{e_n} + h\lambda\theta\norm{e_n} - h\lambda(1-\theta)\norm{e_{n+1}} + ch^2\\
	&\le \frac{1+\theta h\lambda}{1-(1-\theta)h\lambda} \norm{e_n} + \frac{c}{1-(1-\theta)h\lambda}h^2
	\end{align*}
	Assume that
	\[
	\norm{e_n} \le \frac{c}{\lambda} \pa{\pa{\frac{1+\theta h\lambda}{1-(1-\theta)h\lambda}}^n - 1}h
	\]
	By induction, $e_0 = 0 \le 0$ so it is true for $n=0$. For $n+1$,
	\begin{align*}
	\norm{e_{n+1}} 
	&\le \frac{1+\theta h\lambda}{1-(1-\theta)h\lambda}\cdot \frac{c}{\lambda} \pa{\pa{\frac{1+\theta h\lambda}{1-(1-\theta)h\lambda}}^n - 1}h + \frac{c}{1-(1-\theta)h\lambda}h^2\\
	&= \frac{c}{\lambda} \pa{\pa{\frac{1+\theta h\lambda}{1-(1-\theta)h\lambda}}^{n+1} - 1}h
	\end{align*}
	So the assumption is true. By a Taylor expansion of the exponential function,
	\[
	\frac{1+\theta h\lambda}{1-(1-\theta)h\lambda}
	= 1+\frac{h\lambda}{1-(1-\theta)h\lambda}
	\le \exp\pa{\frac{h\lambda}{1-(1-\theta)h\lambda}}
	\]
	$\norm{e_n}$ can be expressed as 
	\[
	\norm{e_n} 
	\le \frac{ch}{\lambda} \pa{\frac{1+\theta h\lambda}{1-(1-\theta)h\lambda}}^n
	\le \frac{ch}{\lambda} \exp\pa{\frac{nh\lambda}{1-(1-\theta)h\lambda}}
	\le \frac{ch}{\lambda} \exp\pa{\frac{t^*\lambda}{1-(1-\theta)h\lambda}}
	\]
	Therefore,
	\[
	\lim\limits_{\substack{h\to 0\\0\le nh\le t^*}} \norm{e_n} = 0
	\]
	Hence the theta method converges.
	
	\item 
	(Problem 1.4) Given $\theta\in[0,1]$, find the order of the method
	\[
	y_{n+1} = y_n + hf(t_n+(1-\theta)h, \theta y_n + (1-\theta)y_{n+1})
	\]
	
	Denote the error as $\eta$ and substitute the exact solution,
	\[
	y(t_{n+1}) = y(t_n) + hf(t_n+(1-\theta)h, \theta y(t_n) + (1-\theta)y(t_{n+1})) + \eta	\tag{1}
	\]
	Modify $f(\cdot)$ and we have
	\[
	hf(t_n+(1-\theta)h, \theta y(t_n) + (1-\theta)y(t_{n+1}))
	= hf(t_n+(1-\theta)h, y(t_n+(1-\theta)h)) + \eta_1	\tag{2}
	\]
	By Lipschitz condition,
	\begin{align*}
	\abs{\eta_1} 
	&= h\norm{f(t_n+(1-\theta)h, \theta y(t_n) + (1-\theta)y(t_{n+1})) - f(t_n+(1-\theta)h, y(t_n+(1-\theta)h))}\\
	&\le h\lambda \norm{\theta y(t_n) + (1-\theta)y(t_{n+1}) - y(t_n+(1-\theta)h)}
	\end{align*}
	Apply Taylor expansion,
	\[
	\abs{\eta_1} 
	\le h\lambda \norm{\theta y(t_n) + (1-\theta)y(t_n) + (1-\theta)hy'(t_n) - y(t_n) - (1-\theta)hy'(t_n) + O(h^2)}
	= O(h^3)	\tag{3}
	\]
	Plug (2), (3) in (1),
	\begin{align*}
	\abs{\eta}
	&= y(t_{n+1}) - y(t_n) - hy'(t_n + (1-\theta)h) + O(h^3)\\
	&= \pa{y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n)} - y(t_n) - h\pa{y'(t_n) + (1-\theta)hy''(t_n) + O(h^2)} + O(h^3)\\
	&= \pa{\theta-\frac{1}{2}}h^2y''(t_n) + O(h^3)
	\end{align*}
	Therefore the method is of order 2 for $\theta = \frac{1}{2}$ and otherwise of 1.
	
\end{enumerate}

\section*{Computer Assignment}
Implement forward Euler, Taylor series, and Trapezoidal rule methods using Matlab. Note you have to write your own routine. You will NOT get any credits by calling the ode solvers in Matlab. Consider IVP: $\frac{dy}{dt} = 1+(y-t)^2, y(0)=1/2$. Note that the analytical sollution is $y(t)=t+1/(2-t)$. Consider $t\in [0,1]$. Comment on what you get.
\begin{enumerate}
	\item
	Use forward Euler's method to compute approximate values $y_1, y_2, \dots, y_n$ of $y(t)$ at points $t_k=k/n$, for $k=1,2,\dots,n$ and $n = 10,20,40$. Show your results using 4 columns: time step $h$, $u(t_k)$, $y(t_k)-u(t_k)$ and $(y(t_k) - u(t_k))/h$. 
	\lstinputlisting{euler.m}
	\begin{table}[H]
		\centering
		\begin{tabular}{*{4}{N}} 
		\toprule
		h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h \\ \midrule
		0.100 & 0.625000 & 0.001316 & 0.013158\\
		0.100 & 0.752563 & 0.002993 & 0.029931\\
		0.100 & 0.883095 & 0.005140 & 0.051403\\
		0.100 & 1.017095 & 0.007905 & 0.079050\\
		0.100 & 1.155176 & 0.011491 & 0.114910\\
		0.100 & 1.298101 & 0.016185 & 0.161846\\
		0.100 & 1.446836 & 0.022395 & 0.223951\\
		0.100 & 1.602612 & 0.030721 & 0.307213\\
		0.100 & 1.767031 & 0.042060 & 0.420603\\
		0.100 & 1.942205 & 0.057795 & 0.577952\\
		\bottomrule
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\begin{tabular}{*{3}{N} M{6em}}
		\toprule
		h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h \\ \midrule
		0.050 & 0.562500 & 0.000321 & 0.006410\\
		0.050 & 0.625633 & 0.000683 & 0.013660\\
		0.050 & 0.689447 & 0.001093 & 0.021865\\
		0.050 & 0.753997 & 0.001558 & 0.031162\\
		0.050 & 0.819343 & 0.002085 & 0.041709\\
		0.050 & 0.885551 & 0.002685 & 0.053692\\
		0.050 & 0.952694 & 0.003366 & 0.067328\\
		0.050 & 1.020856 & 0.004144 & 0.082876\\
		0.050 & 1.090129 & 0.005032 & 0.100639\\
		0.050 & 1.160618 & 0.006049 & 0.120981\\
		\bottomrule
		\end{tabular}
		\begin{tabular}{*{3}{N} M{6em}} 
		\toprule
		h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h \\ \midrule
		0.050 & 1.232438 & 0.007217 & 0.144336\\
		0.050 & 1.305725 & 0.008561 & 0.171224\\
		0.050 & 1.380627 & 0.010114 & 0.202278\\
		0.050 & 1.457318 & 0.011913 & 0.238263\\
		0.050 & 1.535994 & 0.014006 & 0.280117\\
		0.050 & 1.616883 & 0.016450 & 0.328997\\
		0.050 & 1.700248 & 0.019317 & 0.386336\\
		0.050 & 1.786395 & 0.022696 & 0.453928\\
		0.050 & 1.875679 & 0.026702 & 0.534033\\
		0.050 & 1.968523 & 0.031477 & 0.629532\\
		\bottomrule
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\begin{tabular}{*{3}{N} M{6em}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h \\ \midrule
			0.025 & 0.531250 & 0.000079 & 0.003165\\
			0.025 & 0.562657 & 0.000163 & 0.006531\\
			0.025 & 0.594228 & 0.000253 & 0.010114\\
			0.025 & 0.625968 & 0.000348 & 0.013928\\
			0.025 & 0.657884 & 0.000450 & 0.017988\\
			0.025 & 0.689983 & 0.000558 & 0.022311\\
			0.025 & 0.722272 & 0.000673 & 0.026916\\
			0.025 & 0.754760 & 0.000796 & 0.031823\\
			0.025 & 0.787454 & 0.000926 & 0.037054\\
			0.025 & 0.820363 & 0.001066 & 0.042631\\
			0.025 & 0.853496 & 0.001214 & 0.048580\\
			0.025 & 0.886862 & 0.001373 & 0.054929\\
			0.025 & 0.920472 & 0.001543 & 0.061707\\
			0.025 & 0.954337 & 0.001724 & 0.068947\\
			0.025 & 0.988468 & 0.001917 & 0.076684\\
			0.025 & 1.022876 & 0.002124 & 0.084957\\
			0.025 & 1.057575 & 0.002345 & 0.093808\\
			0.025 & 1.092579 & 0.002582 & 0.103282\\
			0.025 & 1.127902 & 0.002836 & 0.113431\\
			0.025 & 1.163559 & 0.003108 & 0.124308\\
			\bottomrule
		\end{tabular}
		\begin{tabular}{*{3}{N} M{6em}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h \\ \midrule
			0.025 & 1.199567 & 0.003399 & 0.135975\\
			0.025 & 1.235943 & 0.003712 & 0.148498\\
			0.025 & 1.272706 & 0.004049 & 0.161949\\
			0.025 & 1.309875 & 0.004410 & 0.176409\\
			0.025 & 1.347474 & 0.004799 & 0.191966\\
			0.025 & 1.385523 & 0.005218 & 0.208719\\
			0.025 & 1.424048 & 0.005669 & 0.226775\\
			0.025 & 1.463074 & 0.006156 & 0.246254\\
			0.025 & 1.502631 & 0.006682 & 0.267289\\
			0.025 & 1.542749 & 0.007251 & 0.290030\\
			0.025 & 1.583461 & 0.007866 & 0.314640\\
			0.025 & 1.624801 & 0.008533 & 0.341303\\
			0.025 & 1.666808 & 0.009256 & 0.370227\\
			0.025 & 1.709524 & 0.010041 & 0.401641\\
			0.025 & 1.752994 & 0.010895 & 0.435806\\
			0.025 & 1.797266 & 0.011825 & 0.473014\\
			0.025 & 1.842393 & 0.012840 & 0.513595\\
			0.025 & 1.888433 & 0.013948 & 0.557921\\
			0.025 & 1.935449 & 0.015160 & 0.606417\\
			0.025 & 1.983511 & 0.016489 & 0.659564\\
			\bottomrule
		\end{tabular}
	\end{table}
	As step $h$ decreases, the error $y(t_k) - u(t_k)$ for same $t_k$ decreases. As $t_k$ increases, the error increases because it is accumulating.
	
	\item
	Redo part 1 using three-term Taylor series method. Note that the last column is replaced by $(y(t_k) - u(t_k))/h^2$. 
	\lstinputlisting{taylor.m}
	\begin{table}[H]
		\centering
		\begin{tabular}{*{4}{N}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.100 & 0.626250 & 0.000066 & 0.006579\\
			0.100 & 0.755401 & 0.000154 & 0.015426\\
			0.100 & 0.887962 & 0.000274 & 0.027369\\
			0.100 & 1.024564 & 0.000436 & 0.043593\\
			0.100 & 1.166008 & 0.000658 & 0.065827\\
			0.100 & 1.313319 & 0.000966 & 0.096640\\
			0.100 & 1.467831 & 0.001399 & 0.139947\\
			0.100 & 1.631315 & 0.002019 & 0.201868\\
			0.100 & 1.806168 & 0.002923 & 0.292277\\
			0.100 & 1.995723 & 0.004277 & 0.427687\\
			\bottomrule
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\begin{tabular}{*{3}{N} M{6em}}
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.050 & 0.562813 & 0.000008 & 0.003205\\
			0.050 & 0.626298 & 0.000017 & 0.006925\\
			0.050 & 0.690512 & 0.000028 & 0.011244\\
			0.050 & 0.755515 & 0.000041 & 0.016264\\
			0.050 & 0.821373 & 0.000055 & 0.022104\\
			0.050 & 0.888163 & 0.000072 & 0.028909\\
			0.050 & 0.955968 & 0.000092 & 0.036852\\
			0.050 & 1.024885 & 0.000115 & 0.046144\\
			0.050 & 1.095019 & 0.000143 & 0.057038\\
			0.050 & 1.166492 & 0.000175 & 0.069847\\
			\bottomrule
		\end{tabular}
		\begin{tabular}{*{3}{N} M{6em}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.050 & 1.239443 & 0.000212 & 0.084953\\
			0.050 & 1.314029 & 0.000257 & 0.102828\\
			0.050 & 1.390431 & 0.000310 & 0.124063\\
			0.050 & 1.468857 & 0.000373 & 0.149393\\
			0.050 & 1.549551 & 0.000449 & 0.179752\\
			0.050 & 1.632793 & 0.000541 & 0.216323\\
			0.050 & 1.718914 & 0.000652 & 0.260631\\
			0.050 & 1.808304 & 0.000787 & 0.314651\\
			0.050 & 1.901429 & 0.000952 & 0.380968\\
			0.050 & 1.998842 & 0.001158 & 0.463014\\
			\bottomrule
		\end{tabular}
	\end{table}
	\begin{table}[H]
		\centering
		\begin{tabular}{*{3}{N} M{6em}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.025 & 0.531328 & 0.000001 & 0.001582\\
			0.025 & 0.562818 & 0.000002 & 0.003287\\
			0.025 & 0.594477 & 0.000003 & 0.005125\\
			0.025 & 0.626311 & 0.000004 & 0.007105\\
			0.025 & 0.658328 & 0.000006 & 0.009240\\
			0.025 & 0.690533 & 0.000007 & 0.011541\\
			0.025 & 0.722936 & 0.000009 & 0.014023\\
			0.025 & 0.755545 & 0.000010 & 0.016700\\
			0.025 & 0.788368 & 0.000012 & 0.019588\\
			0.025 & 0.821414 & 0.000014 & 0.022706\\
			0.025 & 0.854694 & 0.000016 & 0.026073\\
			0.025 & 0.888217 & 0.000019 & 0.029710\\
			0.025 & 0.921994 & 0.000021 & 0.033641\\
			0.025 & 0.956037 & 0.000024 & 0.037892\\
			0.025 & 0.990358 & 0.000027 & 0.042491\\
			0.025 & 1.024970 & 0.000030 & 0.047470\\
			0.025 & 1.059888 & 0.000033 & 0.052863\\
			0.025 & 1.095125 & 0.000037 & 0.058710\\
			0.025 & 1.130697 & 0.000041 & 0.065052\\
			0.025 & 1.166622 & 0.000045 & 0.071936\\
			\bottomrule
		\end{tabular}
		\begin{tabular}{*{3}{N} M{6em}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.025 & 1.202916 & 0.000050 & 0.079416\\
			0.025 & 1.239600 & 0.000055 & 0.087549\\
			0.025 & 1.276694 & 0.000060 & 0.096401\\
			0.025 & 1.314219 & 0.000066 & 0.106043\\
			0.025 & 1.352200 & 0.000073 & 0.116558\\
			0.025 & 1.390661 & 0.000080 & 0.128035\\
			0.025 & 1.429629 & 0.000088 & 0.140577\\
			0.025 & 1.469134 & 0.000096 & 0.154299\\
			0.025 & 1.509208 & 0.000106 & 0.169329\\
			0.025 & 1.549884 & 0.000116 & 0.185814\\
			0.025 & 1.591199 & 0.000127 & 0.203918\\
			0.025 & 1.633193 & 0.000140 & 0.223828\\
			0.025 & 1.675910 & 0.000154 & 0.245758\\
			0.025 & 1.719396 & 0.000169 & 0.269949\\
			0.025 & 1.763703 & 0.000185 & 0.296679\\
			0.025 & 1.808887 & 0.000204 & 0.326265\\
			0.025 & 1.855008 & 0.000224 & 0.359071\\
			0.025 & 1.902134 & 0.000247 & 0.395518\\
			0.025 & 1.950337 & 0.000273 & 0.436092\\
			0.025 & 1.999699 & 0.000301 & 0.481355\\
			\bottomrule
		\end{tabular}
	\end{table}
	As step $h$ decreases, the error $y(t_k) - u(t_k)$ for same $t_k$ decreases. As $t_k$ increases, the error increases because it is accumulating. The error of three term Taylor series method is better than that of Euler's method. 
	
	\item 
	Redo part 2 using Trapezoidal rule together with the predictor-corrector scheme (using explicit Euler to generate preliminary value).
	\lstinputlisting{trapezoidal.m}
	\begin{table}[H]
		\centering
		\vspace{-2ex}
		\begin{tabular}{*{4}{N}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.100 & 0.626281 & 0.000035 & 0.003454\\
			0.100 & 0.755474 & 0.000081 & 0.008110\\
			0.100 & 0.888091 & 0.000144 & 0.014412\\
			0.100 & 1.024770 & 0.000230 & 0.022998\\
			0.100 & 1.166319 & 0.000348 & 0.034800\\
			0.100 & 1.313774 & 0.000512 & 0.051210\\
			0.100 & 1.468487 & 0.000744 & 0.074362\\
			0.100 & 1.632257 & 0.001076 & 0.107607\\
			0.100 & 1.807527 & 0.001564 & 0.156390\\
			0.100 & 1.997701 & 0.002299 & 0.229886\\
			\bottomrule
		\end{tabular}
		\\\vspace{3ex}
		\begin{tabular}{*{3}{N} M{6em}}
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.050 & 0.562816 & 0.000004 & 0.001643\\
			0.050 & 0.626307 & 0.000009 & 0.003550\\
			0.050 & 0.690526 & 0.000014 & 0.005767\\
			0.050 & 0.755535 & 0.000021 & 0.008344\\
			0.050 & 0.821400 & 0.000028 & 0.011345\\
			0.050 & 0.888198 & 0.000037 & 0.014844\\
			0.050 & 0.956013 & 0.000047 & 0.018931\\
			0.050 & 1.024941 & 0.000059 & 0.023715\\
			0.050 & 1.095088 & 0.000073 & 0.029329\\
			0.050 & 1.166577 & 0.000090 & 0.035934\\
			\bottomrule
		\end{tabular}
		\begin{tabular}{*{3}{N} M{6em}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.050 & 1.239546 & 0.000109 & 0.043731\\
			0.050 & 1.314153 & 0.000132 & 0.052966\\
			0.050 & 1.390581 & 0.000160 & 0.063946\\
			0.050 & 1.469038 & 0.000193 & 0.077057\\
			0.050 & 1.549768 & 0.000232 & 0.092788\\
			0.050 & 1.633054 & 0.000279 & 0.111761\\
			0.050 & 1.719228 & 0.000337 & 0.134775\\
			0.050 & 1.808684 & 0.000407 & 0.162872\\
			0.050 & 1.901887 & 0.000494 & 0.197417\\
			0.050 & 1.999399 & 0.000601 & 0.240223\\
			\bottomrule
		\end{tabular}
		\\\vspace{3ex}
		\begin{tabular}{*{3}{N} M{6em}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.025 & 0.531329 & 0.000001 & 0.000801\\
			0.025 & 0.562819 & 0.000001 & 0.001664\\
			0.025 & 0.594479 & 0.000002 & 0.002595\\
			0.025 & 0.626314 & 0.000002 & 0.003598\\
			0.025 & 0.658330 & 0.000003 & 0.004679\\
			0.025 & 0.690537 & 0.000004 & 0.005845\\
			0.025 & 0.722941 & 0.000004 & 0.007103\\
			0.025 & 0.755550 & 0.000005 & 0.008459\\
			0.025 & 0.788374 & 0.000006 & 0.009924\\
			0.025 & 0.821421 & 0.000007 & 0.011504\\
			0.025 & 0.854702 & 0.000008 & 0.013212\\
			0.025 & 0.888226 & 0.000009 & 0.015056\\
			0.025 & 0.922004 & 0.000011 & 0.017050\\
			0.025 & 0.956049 & 0.000012 & 0.019207\\
			0.025 & 0.990371 & 0.000013 & 0.021540\\
			0.025 & 1.024985 & 0.000015 & 0.024067\\
			0.025 & 1.059904 & 0.000017 & 0.026805\\
			0.025 & 1.095143 & 0.000019 & 0.029774\\
			0.025 & 1.130717 & 0.000021 & 0.032994\\
			0.025 & 1.166644 & 0.000023 & 0.036491\\
			\bottomrule
		\end{tabular}
		\begin{tabular}{*{3}{N} M{6em}} 
			\toprule
			h & u(t_k) & y(t_k)-u(t_k) & (y(t_k)-u(t_k))/h^2 \\ \midrule
			0.025 & 1.202941 & 0.000025 & 0.040291\\
			0.025 & 1.239627 & 0.000028 & 0.044424\\
			0.025 & 1.276724 & 0.000031 & 0.048922\\
			0.025 & 1.314252 & 0.000034 & 0.053824\\
			0.025 & 1.352236 & 0.000037 & 0.059171\\
			0.025 & 1.390700 & 0.000041 & 0.065008\\
			0.025 & 1.429672 & 0.000045 & 0.071389\\
			0.025 & 1.469182 & 0.000049 & 0.078372\\
			0.025 & 1.509260 & 0.000054 & 0.086022\\
			0.025 & 1.549941 & 0.000059 & 0.094415\\
			0.025 & 1.591262 & 0.000065 & 0.103636\\
			0.025 & 1.633262 & 0.000071 & 0.113779\\
			0.025 & 1.675986 & 0.000078 & 0.124954\\
			0.025 & 1.719479 & 0.000086 & 0.137286\\
			0.025 & 1.763795 & 0.000094 & 0.150917\\
			0.025 & 1.808987 & 0.000104 & 0.166009\\
			0.025 & 1.855118 & 0.000114 & 0.182751\\
			0.025 & 1.902255 & 0.000126 & 0.201357\\
			0.025 & 1.950471 & 0.000139 & 0.222078\\
			0.025 & 1.999847 & 0.000153 & 0.245204\\
			\bottomrule
		\end{tabular}
		\vspace{-2ex}
	\end{table}
	As step $h$ decreases, the error $y(t_k) - u(t_k)$ for same $t_k$ decreases. As $t_k$ increases, the error increases because it is accumulating. The error of Trapezoidal rule is a little worse than that of three term Taylor series method. But the $(y(t_k)-u(t_k)/h^2$ term is better than that of three term Taylor series method.
\end{enumerate}

\end{document}


